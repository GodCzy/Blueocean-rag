#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
fine_tuning.py - å¾®è°ƒè®­ç»ƒæ¨¡å—

é’ˆå¯¹æ°´ç”ŸåŠ¨ç‰©ç–¾ç—…è¯Šæ–­çš„ä¸“ç”¨å¾®è°ƒè®­ç»ƒæ¨¡å—

ä½œè€…: è“æµ·æ™ºè¯¢å›¢é˜Ÿ
"""

import os
import json
import torch
import asyncio
from typing import Dict, List, Any, Optional, Union
from pathlib import Path
from dataclasses import dataclass, asdict
from datetime import datetime
import pandas as pd
from transformers import (
    AutoTokenizer, AutoModelForCausalLM,
    TrainingArguments, Trainer,
    DataCollatorForLanguageModeling,
    EarlyStoppingCallback
)
from peft import LoraConfig, TaskType, get_peft_model, PeftModel
from datasets import Dataset, load_dataset
import numpy as np
from sklearn.model_selection import train_test_split

from ..config.settings import get_settings
from ..utils.logger import get_logger
from .oceangpt_manager import OceanGPTManager, ModelConfig

logger = get_logger(__name__)
settings = get_settings()

@dataclass
class FineTuningConfig:
    """å¾®è°ƒé…ç½®"""
    # æ¨¡å‹é…ç½®
    base_model_name: str = "OceanGPT-o-7B-v0.1"
    output_dir: str = "./saves/fine_tuned_models"
    
    # LoRAé…ç½®
    lora_r: int = 64
    lora_alpha: int = 16
    lora_dropout: float = 0.1
    lora_target_modules: List[str] = None
    
    # è®­ç»ƒå‚æ•°
    num_train_epochs: int = 3
    per_device_train_batch_size: int = 1
    per_device_eval_batch_size: int = 1
    gradient_accumulation_steps: int = 8
    learning_rate: float = 1e-4
    weight_decay: float = 0.01
    lr_scheduler_type: str = "cosine"
    warmup_steps: int = 100
    max_seq_length: int = 2048
    
    # è®­ç»ƒæ§åˆ¶
    save_steps: int = 500
    eval_steps: int = 500
    logging_steps: int = 50
    early_stopping_patience: int = 3
    
    # æ•°æ®é…ç½®
    train_test_split: float = 0.2
    seed: int = 42
    
    # é«˜çº§é…ç½®
    use_flash_attention: bool = True
    use_gradient_checkpointing: bool = True
    fp16: bool = True
    bf16: bool = False

@dataclass
class DiagnosisExample:
    """ç–¾ç—…è¯Šæ–­ç¤ºä¾‹"""
    animal_type: str  # åŠ¨ç‰©ç±»å‹
    symptoms: List[str]  # ç—‡çŠ¶åˆ—è¡¨
    environment: Dict[str, Any]  # ç¯å¢ƒå‚æ•°
    diagnosis: str  # è¯Šæ–­ç»“æœ
    treatment: str  # æ²»ç–—æ–¹æ¡ˆ
    confidence: float = 1.0  # ç½®ä¿¡åº¦
    source: str = "manual"  # æ•°æ®æ¥æº

class MarineDiagnosisTrainer:
    """æ°´ç”ŸåŠ¨ç‰©ç–¾ç—…è¯Šæ–­è®­ç»ƒå™¨"""
    
    def __init__(self, config: FineTuningConfig = None):
        self.config = config or FineTuningConfig()
        self.tokenizer = None
        self.model = None
        self.trainer = None
        self.training_data = []
        
        # è®¾ç½®é»˜è®¤target_modules
        if self.config.lora_target_modules is None:
            self.config.lora_target_modules = [
                "q_proj", "v_proj", "k_proj", "o_proj",
                "gate_proj", "up_proj", "down_proj"
            ]
    
    async def load_base_model(self):
        """åŠ è½½åŸºç¡€æ¨¡å‹"""
        logger.info(f"åŠ è½½åŸºç¡€æ¨¡å‹: {self.config.base_model_name}")
        
        try:
            # ä½¿ç”¨OceanGPTç®¡ç†å™¨åŠ è½½æ¨¡å‹
            model_config = ModelConfig(
                model_name=self.config.base_model_name,
                device="auto",
                load_in_4bit=True
            )
            
            manager = OceanGPTManager(model_config)
            success = await manager.load_model()
            
            if not success:
                raise ValueError("åŸºç¡€æ¨¡å‹åŠ è½½å¤±è´¥")
            
            self.tokenizer = manager.tokenizer or manager.processor.tokenizer
            self.model = manager.model
            
            logger.info("åŸºç¡€æ¨¡å‹åŠ è½½æˆåŠŸ")
            return True
            
        except Exception as e:
            logger.error(f"åŠ è½½åŸºç¡€æ¨¡å‹å¤±è´¥: {e}")
            return False
    
    def load_marine_disease_data(self, data_path: str = None) -> List[DiagnosisExample]:
        """åŠ è½½æ°´ç”ŸåŠ¨ç‰©ç–¾ç—…æ•°æ®"""
        if not data_path:
            data_path = os.path.join(settings.data_dir, "marine_diseases")
        
        data_path = Path(data_path)
        examples = []
        
        try:
            # åŠ è½½ä¸åŒæ ¼å¼çš„æ•°æ®æ–‡ä»¶
            for file_path in data_path.glob("**/*"):
                if file_path.suffix.lower() in ['.json', '.jsonl']:
                    examples.extend(self._load_json_data(file_path))
                elif file_path.suffix.lower() in ['.csv']:
                    examples.extend(self._load_csv_data(file_path))
                elif file_path.suffix.lower() in ['.xlsx', '.xls']:
                    examples.extend(self._load_excel_data(file_path))
            
            logger.info(f"åŠ è½½ç–¾ç—…è¯Šæ–­æ•°æ®: {len(examples)} æ¡")
            return examples
            
        except Exception as e:
            logger.error(f"åŠ è½½ç–¾ç—…æ•°æ®å¤±è´¥: {e}")
            return []
    
    def _load_json_data(self, file_path: Path) -> List[DiagnosisExample]:
        """åŠ è½½JSONæ ¼å¼æ•°æ®"""
        examples = []
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                if file_path.suffix == '.jsonl':
                    data = [json.loads(line) for line in f]
                else:
                    data = json.load(f)
                
                if not isinstance(data, list):
                    data = [data]
                
                for item in data:
                    example = self._convert_to_diagnosis_example(item)
                    if example:
                        examples.append(example)
        
        except Exception as e:
            logger.error(f"è¯»å–JSONæ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        
        return examples
    
    def _load_csv_data(self, file_path: Path) -> List[DiagnosisExample]:
        """åŠ è½½CSVæ ¼å¼æ•°æ®"""
        examples = []
        
        try:
            df = pd.read_csv(file_path)
            
            for _, row in df.iterrows():
                example = self._convert_to_diagnosis_example(row.to_dict())
                if example:
                    examples.append(example)
        
        except Exception as e:
            logger.error(f"è¯»å–CSVæ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        
        return examples
    
    def _load_excel_data(self, file_path: Path) -> List[DiagnosisExample]:
        """åŠ è½½Excelæ ¼å¼æ•°æ®"""
        examples = []
        
        try:
            df = pd.read_excel(file_path)
            
            for _, row in df.iterrows():
                example = self._convert_to_diagnosis_example(row.to_dict())
                if example:
                    examples.append(example)
        
        except Exception as e:
            logger.error(f"è¯»å–Excelæ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        
        return examples
    
    def _convert_to_diagnosis_example(self, data: dict) -> Optional[DiagnosisExample]:
        """è½¬æ¢æ•°æ®ä¸ºè¯Šæ–­ç¤ºä¾‹æ ¼å¼"""
        try:
            # æ”¯æŒå¤šç§æ•°æ®æ ¼å¼
            if 'animal_type' in data and 'symptoms' in data:
                # æ ‡å‡†æ ¼å¼
                return DiagnosisExample(
                    animal_type=data.get('animal_type', ''),
                    symptoms=data.get('symptoms', []) if isinstance(data.get('symptoms'), list) 
                            else data.get('symptoms', '').split(',') if data.get('symptoms') else [],
                    environment=data.get('environment', {}) if isinstance(data.get('environment'), dict)
                               else json.loads(data.get('environment', '{}')) if data.get('environment') else {},
                    diagnosis=data.get('diagnosis', ''),
                    treatment=data.get('treatment', ''),
                    confidence=float(data.get('confidence', 1.0)),
                    source=data.get('source', 'imported')
                )
            
            elif 'instruction' in data and 'output' in data:
                # OceanInstructæ ¼å¼
                instruction = data['instruction']
                output = data['output']
                
                # å°è¯•ä»æŒ‡ä»¤ä¸­æå–ä¿¡æ¯
                animal_type = self._extract_animal_type(instruction)
                symptoms = self._extract_symptoms(instruction)
                environment = self._extract_environment(instruction)
                
                return DiagnosisExample(
                    animal_type=animal_type,
                    symptoms=symptoms,
                    environment=environment,
                    diagnosis=output,
                    treatment=data.get('treatment', ''),
                    source='ocean_instruct'
                )
        
        except Exception as e:
            logger.warning(f"è½¬æ¢æ•°æ®æ ¼å¼å¤±è´¥: {e}")
        
        return None
    
    def _extract_animal_type(self, text: str) -> str:
        """ä»æ–‡æœ¬ä¸­æå–åŠ¨ç‰©ç±»å‹"""
        animals = ['é±¼', 'è™¾', 'èŸ¹', 'è´ç±»', 'æµ·å‚', 'æµ·å¸¦', 'é²é±¼', 'æ‰‡è´', 'ç‰¡è›']
        for animal in animals:
            if animal in text:
                return animal
        return 'é±¼ç±»'  # é»˜è®¤
    
    def _extract_symptoms(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–ç—‡çŠ¶"""
        symptoms_keywords = [
            'æ­»äº¡', 'æ¸¸æ³³å¼‚å¸¸', 'é£Ÿæ¬²ä¸æŒ¯', 'ä½“è¡¨æºƒçƒ‚', 'é³ƒä¸å‘ç™½',
            'ä½“è‰²å¼‚å¸¸', 'è…¹èƒ€', 'çœ¼çƒçªå‡º', 'é³ç‰‡è„±è½', 'ç™½ç‚¹'
        ]
        
        found_symptoms = []
        for symptom in symptoms_keywords:
            if symptom in text:
                found_symptoms.append(symptom)
        
        return found_symptoms or ['æœªæ˜ç¡®ç—‡çŠ¶']
    
    def _extract_environment(self, text: str) -> Dict[str, Any]:
        """ä»æ–‡æœ¬ä¸­æå–ç¯å¢ƒä¿¡æ¯"""
        import re
        
        environment = {}
        
        # æå–æ¸©åº¦
        temp_match = re.search(r'æ¸©åº¦[ï¼š:]\s*(\d+(?:\.\d+)?)[Â°â„ƒ]?', text)
        if temp_match:
            environment['temperature'] = float(temp_match.group(1))
        
        # æå–pHå€¼
        ph_match = re.search(r'pH[å€¼]?[ï¼š:]\s*(\d+(?:\.\d+)?)', text)
        if ph_match:
            environment['ph'] = float(ph_match.group(1))
        
        # æå–æº¶æ°§
        do_match = re.search(r'æº¶æ°§[ï¼š:]\s*(\d+(?:\.\d+)?)', text)
        if do_match:
            environment['dissolved_oxygen'] = float(do_match.group(1))
        
        return environment
    
    def prepare_training_data(self, examples: List[DiagnosisExample]) -> Dataset:
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        logger.info("å‡†å¤‡è®­ç»ƒæ•°æ®...")
        
        formatted_data = []
        
        for example in examples:
            # æ„å»ºè®­ç»ƒæ–‡æœ¬
            prompt = self._build_diagnosis_prompt(example)
            response = self._build_diagnosis_response(example)
            
            # ç»„åˆä¸ºå¯¹è¯æ ¼å¼
            text = f"<|user|>\n{prompt}\n<|assistant|>\n{response}<|end|>"
            
            formatted_data.append({
                "text": text,
                "animal_type": example.animal_type,
                "confidence": example.confidence
            })
        
        # åˆ›å»ºDataset
        dataset = Dataset.from_list(formatted_data)
        
        logger.info(f"è®­ç»ƒæ•°æ®å‡†å¤‡å®Œæˆ: {len(formatted_data)} æ¡")
        return dataset
    
    def _build_diagnosis_prompt(self, example: DiagnosisExample) -> str:
        """æ„å»ºè¯Šæ–­æç¤º"""
        prompt_parts = [
            f"è¯·ä¸ºä»¥ä¸‹{example.animal_type}çš„ç–¾ç—…ç—‡çŠ¶è¿›è¡Œè¯Šæ–­ï¼š",
            f"è§‚å¯Ÿåˆ°çš„ç—‡çŠ¶ï¼š{', '.join(example.symptoms)}"
        ]
        
        if example.environment:
            env_desc = []
            for key, value in example.environment.items():
                env_desc.append(f"{key}: {value}")
            prompt_parts.append(f"ç¯å¢ƒå‚æ•°ï¼š{', '.join(env_desc)}")
        
        prompt_parts.append("è¯·æä¾›è¯¦ç»†çš„è¯Šæ–­åˆ†æå’Œæ²»ç–—å»ºè®®ã€‚")
        
        return "\n".join(prompt_parts)
    
    def _build_diagnosis_response(self, example: DiagnosisExample) -> str:
        """æ„å»ºè¯Šæ–­å›å¤"""
        response_parts = [f"è¯Šæ–­ç»“æœï¼š{example.diagnosis}"]
        
        if example.treatment:
            response_parts.append(f"æ²»ç–—æ–¹æ¡ˆï¼š{example.treatment}")
        
        return "\n".join(response_parts)
    
    def setup_lora_config(self) -> LoraConfig:
        """è®¾ç½®LoRAé…ç½®"""
        return LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=self.config.lora_r,
            lora_alpha=self.config.lora_alpha,
            lora_dropout=self.config.lora_dropout,
            target_modules=self.config.lora_target_modules,
            bias="none"
        )
    
    def data_collator(self, features):
        """æ•°æ®æ•´ç†å‡½æ•°"""
        texts = [feature["text"] for feature in features]
        
        # Tokenize
        batch = self.tokenizer(
            texts,
            padding=True,
            truncation=True,
            max_length=self.config.max_seq_length,
            return_tensors="pt"
        )
        
        # è®¾ç½®labels
        batch["labels"] = batch["input_ids"].clone()
        
        return batch
    
    def compute_metrics(self, eval_pred):
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        predictions, labels = eval_pred
        
        # è®¡ç®—å›°æƒ‘åº¦
        predictions = predictions.reshape(-1, predictions.shape[-1])
        labels = labels.reshape(-1)
        
        # å¿½ç•¥padding token
        mask = labels != -100
        predictions = predictions[mask]
        labels = labels[mask]
        
        # è®¡ç®—äº¤å‰ç†µæŸå¤±
        import torch.nn.functional as F
        loss = F.cross_entropy(torch.tensor(predictions), torch.tensor(labels))
        perplexity = torch.exp(loss).item()
        
        return {
            "perplexity": perplexity,
            "eval_loss": loss.item()
        }
    
    async def start_training(self, data_path: str = None) -> bool:
        """å¼€å§‹è®­ç»ƒ"""
        logger.info("ğŸš€ å¼€å§‹æ°´ç”ŸåŠ¨ç‰©ç–¾ç—…è¯Šæ–­æ¨¡å‹å¾®è°ƒè®­ç»ƒ")
        
        try:
            # 1. åŠ è½½åŸºç¡€æ¨¡å‹
            if not await self.load_base_model():
                return False
            
            # 2. åŠ è½½è®­ç»ƒæ•°æ®
            examples = self.load_marine_disease_data(data_path)
            if not examples:
                logger.error("æ— æ³•åŠ è½½è®­ç»ƒæ•°æ®")
                return False
            
            # 3. å‡†å¤‡è®­ç»ƒæ•°æ®
            dataset = self.prepare_training_data(examples)
            
            # 4. æ•°æ®åˆ†å‰²
            train_dataset, eval_dataset = self._split_dataset(dataset)
            
            # 5. è®¾ç½®LoRA
            lora_config = self.setup_lora_config()
            self.model = get_peft_model(self.model, lora_config)
            self.model.print_trainable_parameters()
            
            # 6. è®¾ç½®è®­ç»ƒå‚æ•°
            training_args = self._setup_training_args()
            
            # 7. åˆ›å»ºè®­ç»ƒå™¨
            self.trainer = Trainer(
                model=self.model,
                args=training_args,
                train_dataset=train_dataset,
                eval_dataset=eval_dataset,
                tokenizer=self.tokenizer,
                data_collator=self.data_collator,
                compute_metrics=self.compute_metrics,
                callbacks=[EarlyStoppingCallback(early_stopping_patience=self.config.early_stopping_patience)]
            )
            
            # 8. å¼€å§‹è®­ç»ƒ
            logger.info("å¼€å§‹è®­ç»ƒ...")
            train_result = self.trainer.train()
            
            # 9. ä¿å­˜æ¨¡å‹
            self.trainer.save_model()
            
            # 10. ä¿å­˜è®­ç»ƒæ—¥å¿—
            self._save_training_log(train_result)
            
            logger.info("âœ… è®­ç»ƒå®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"è®­ç»ƒå¤±è´¥: {e}")
            return False
    
    def _split_dataset(self, dataset: Dataset):
        """åˆ†å‰²æ•°æ®é›†"""
        # éšæœºåˆ†å‰²
        train_indices, eval_indices = train_test_split(
            range(len(dataset)),
            test_size=self.config.train_test_split,
            random_state=self.config.seed
        )
        
        train_dataset = dataset.select(train_indices)
        eval_dataset = dataset.select(eval_indices)
        
        logger.info(f"æ•°æ®åˆ†å‰²å®Œæˆ - è®­ç»ƒé›†: {len(train_dataset)}, éªŒè¯é›†: {len(eval_dataset)}")
        
        return train_dataset, eval_dataset
    
    def _setup_training_args(self) -> TrainingArguments:
        """è®¾ç½®è®­ç»ƒå‚æ•°"""
        output_dir = os.path.join(
            self.config.output_dir,
            f"marine_diagnosis_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        )
        
        return TrainingArguments(
            output_dir=output_dir,
            overwrite_output_dir=True,
            num_train_epochs=self.config.num_train_epochs,
            per_device_train_batch_size=self.config.per_device_train_batch_size,
            per_device_eval_batch_size=self.config.per_device_eval_batch_size,
            gradient_accumulation_steps=self.config.gradient_accumulation_steps,
            learning_rate=self.config.learning_rate,
            weight_decay=self.config.weight_decay,
            lr_scheduler_type=self.config.lr_scheduler_type,
            warmup_steps=self.config.warmup_steps,
            logging_steps=self.config.logging_steps,
            save_steps=self.config.save_steps,
            eval_steps=self.config.eval_steps,
            evaluation_strategy="steps",
            save_strategy="steps",
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            remove_unused_columns=False,
            dataloader_pin_memory=False,
            fp16=self.config.fp16,
            bf16=self.config.bf16,
            gradient_checkpointing=self.config.use_gradient_checkpointing,
            report_to=None,  # ä¸ä½¿ç”¨wandbç­‰
            run_name=f"marine_diagnosis_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        )
    
    def _save_training_log(self, train_result):
        """ä¿å­˜è®­ç»ƒæ—¥å¿—"""
        log_data = {
            "config": asdict(self.config),
            "train_result": {
                "training_loss": train_result.training_loss,
                "train_runtime": train_result.metrics.get("train_runtime"),
                "train_samples_per_second": train_result.metrics.get("train_samples_per_second"),
                "epoch": train_result.metrics.get("epoch")
            },
            "timestamp": datetime.now().isoformat()
        }
        
        log_file = os.path.join(self.config.output_dir, "training_log.json")
        os.makedirs(os.path.dirname(log_file), exist_ok=True)
        
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump(log_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"è®­ç»ƒæ—¥å¿—å·²ä¿å­˜: {log_file}")

class MarineDiagnosisDataGenerator:
    """æ°´ç”ŸåŠ¨ç‰©ç–¾ç—…è¯Šæ–­æ•°æ®ç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.common_diseases = {
            "é±¼ç±»": [
                {"name": "ç™½ç‚¹ç—…", "symptoms": ["ä½“è¡¨ç™½ç‚¹", "æ¸¸æ³³å¼‚å¸¸", "æ“¦èº«è¡Œä¸º"], "treatment": "ä½¿ç”¨ç”²åŸºè“è¯æµ´"},
                {"name": "çƒ‚é³ƒç—…", "symptoms": ["é³ƒä¸å‘ç™½", "å‘¼å¸å›°éš¾", "é£Ÿæ¬²ä¸æŒ¯"], "treatment": "æŠ—ç”Ÿç´ æ²»ç–—"},
                {"name": "è‚ ç‚", "symptoms": ["è…¹èƒ€", "ç²ªä¾¿å¼‚å¸¸", "é£Ÿæ¬²ä¸æŒ¯"], "treatment": "è°ƒæ•´é¥²æ–™ï¼Œä½¿ç”¨ç›Šç”ŸèŒ"}
            ],
            "è™¾ç±»": [
                {"name": "ç™½æ–‘ç—…", "symptoms": ["ç”²å£³ç™½æ–‘", "æ´»åŠ›ä¸‹é™", "æ­»äº¡ç‡é«˜"], "treatment": "æ”¹å–„æ°´è´¨ï¼Œä½¿ç”¨æŠ—ç—…æ¯’è¯ç‰©"},
                {"name": "çƒ‚é³ƒç—…", "symptoms": ["é³ƒéƒ¨å‘é»‘", "æµ®å¤´", "æ­»äº¡"], "treatment": "æ¢æ°´ï¼Œä½¿ç”¨æ¶ˆæ¯’å‰‚"}
            ]
        }
    
    def generate_training_examples(self, count: int = 1000) -> List[DiagnosisExample]:
        """ç”Ÿæˆè®­ç»ƒç¤ºä¾‹"""
        examples = []
        
        for _ in range(count):
            # éšæœºé€‰æ‹©åŠ¨ç‰©ç±»å‹å’Œç–¾ç—…
            animal_type = np.random.choice(list(self.common_diseases.keys()))
            disease = np.random.choice(self.common_diseases[animal_type])
            
            # ç”Ÿæˆç¯å¢ƒå‚æ•°
            environment = self._generate_environment_params()
            
            # åˆ›å»ºç¤ºä¾‹
            example = DiagnosisExample(
                animal_type=animal_type,
                symptoms=disease["symptoms"],
                environment=environment,
                diagnosis=disease["name"],
                treatment=disease["treatment"],
                confidence=np.random.uniform(0.8, 1.0),
                source="generated"
            )
            
            examples.append(example)
        
        return examples
    
    def _generate_environment_params(self) -> Dict[str, Any]:
        """ç”Ÿæˆç¯å¢ƒå‚æ•°"""
        return {
            "temperature": np.random.uniform(20, 30),
            "ph": np.random.uniform(6.5, 8.5),
            "dissolved_oxygen": np.random.uniform(4, 8),
            "salinity": np.random.uniform(20, 35),
            "ammonia": np.random.uniform(0, 0.5)
        }
    
    def save_examples_to_file(self, examples: List[DiagnosisExample], file_path: str):
        """ä¿å­˜ç¤ºä¾‹åˆ°æ–‡ä»¶"""
        data = [asdict(example) for example in examples]
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"å·²ç”Ÿæˆ {len(examples)} ä¸ªè®­ç»ƒç¤ºä¾‹å¹¶ä¿å­˜åˆ°: {file_path}")

# è®­ç»ƒç®¡ç†å™¨
class TrainingManager:
    """è®­ç»ƒç®¡ç†å™¨"""
    
    def __init__(self):
        self.trainer = MarineDiagnosisTrainer()
        self.data_generator = MarineDiagnosisDataGenerator()
    
    async def quick_start_training(self, use_generated_data: bool = True, generated_count: int = 1000):
        """å¿«é€Ÿå¼€å§‹è®­ç»ƒ"""
        logger.info("ğŸŒŠ å¼€å§‹å¿«é€Ÿè®­ç»ƒæ°´ç”ŸåŠ¨ç‰©ç–¾ç—…è¯Šæ–­æ¨¡å‹")
        
        try:
            # å¦‚æœä½¿ç”¨ç”Ÿæˆæ•°æ®ï¼Œå…ˆç”Ÿæˆè®­ç»ƒæ•°æ®
            if use_generated_data:
                logger.info("ç”Ÿæˆè®­ç»ƒæ•°æ®...")
                examples = self.data_generator.generate_training_examples(generated_count)
                
                # ä¿å­˜ç”Ÿæˆçš„æ•°æ®
                os.makedirs(settings.data_dir, exist_ok=True)
                data_file = os.path.join(settings.data_dir, "generated_marine_diseases.json")
                self.data_generator.save_examples_to_file(examples, data_file)
            
            # å¼€å§‹è®­ç»ƒ
            success = await self.trainer.start_training()
            
            if success:
                logger.info("âœ… å¿«é€Ÿè®­ç»ƒå®Œæˆ")
            else:
                logger.error("âŒ å¿«é€Ÿè®­ç»ƒå¤±è´¥")
            
            return success
            
        except Exception as e:
            logger.error(f"å¿«é€Ÿè®­ç»ƒå‡ºé”™: {e}")
            return False 